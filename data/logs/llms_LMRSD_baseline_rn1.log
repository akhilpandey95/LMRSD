nohup: ignoring input
2025-06-05 21:48:23,397 [INFO] __main__ - Polars concurrency set to 12 threads.
2025-06-05 21:48:25,141 [INFO] __main__ - Using data from /home/ubuntu/data
2025-06-05 21:48:25,141 [INFO] __main__ - Loading datasets from parquet files...
2025-06-05 21:48:25,200 [INFO] __main__ - LMRSD dataset length: 28033
2025-06-05 21:48:25,200 [INFO] __main__ - Sample Input from LMRSD dataset:
2025-06-05 21:48:25,201 [INFO] __main__ - TITLE:
Decomposition into invariant spaces with $L_1$-type contrastive learning
2025-06-05 21:48:25,201 [INFO] __main__ - ----------------------------------
2025-06-05 21:48:25,201 [INFO] __main__ - ABSTRACT:
Recent years have witnessed the effectiveness of contrastive learning in obtaining
the representation of dataset that is useful in interpretation and downstream tasks.
However, the mechanism by which the contrastive learning succeeds in this feat
has not been fully uncovered. In this paper, we show that contrastive learning can
uncover a fine decomposition of the dataset into a set of latent features defined by
augmentations, and that such a decomposition can be achieved just by changing
the metric in the simCLR-type loss.
2025-06-05 21:48:25,201 [INFO] __main__ - ----------------------------------
2025-06-05 21:48:25,201 [INFO] __main__ - KEYWORDS:
["Contrastive Learning", "Invariant Space"]
2025-06-05 21:48:25,201 [INFO] __main__ - ----------------------------------
2025-06-05 21:48:25,201 [INFO] __main__ - dict_keys(['paper_id', 'paper_title', 'paper_keywords', 'paper_abstract', 'avg_idea_score', 'avg_idea_score_cf', 'median_idea_score', 'median_idea_score_cf'])
2025-06-05 21:48:25,201 [INFO] __main__ - IDEA Evaluation
: Rating: 1.5, Confidence: 2.0
2025-06-05 21:48:25,458 [INFO] __main__ - Sample prompt:
<|im_start|>system

    You are an expert peer reivew agent working on assessing
    evaluating and giving a review score for the ideas and
    content represented in any given scientific texts.
    <|im_end|>
<|im_start|>user

        **Task:** You are given Paper title, abstract and keywords of a scientific
        paper. Your goal is to accurately analyze the entire manuscript and play the
        role of a peer-reviewer to evaluate the ideas presented in the paper. You need
        to give a numerical score outlining your review and confidence in your decision.

        **Considerations:**
        1. You have to give a review of the idea from the range 1 to 10.
        2. Your rating of the paper's idea must include a confidence on the range of 1 to 10.
        3. You will be given papers across different scientific fields so be adaptable when reviewing the idea.
        4. DONOT hallucinate and produce new information.

        **Response Format:**
        ```json
        {
        'idea_only_review_confidence': int
        'idea_only_review_content': str
        'idea_only_review_rating': int
        }
        ```
        
        **Paper Title:**
        ```plaintext
        Decomposition into invariant spaces with $L_1$-type contrastive learning
        ```

        **Paper Abstract:**
        ```plaintext
        Recent years have witnessed the effectiveness of contrastive learning in obtaining
the representation of dataset that is useful in interpretation and downstream tasks.
However, the mechanism by which the contrastive learning succeeds in this feat
has not been fully uncovered. In this paper, we show that contrastive learning can
uncover a fine decomposition of the dataset into a set of latent features defined by
augmentations, and that such a decomposition can be achieved just by changing
the metric in the simCLR-type loss.
        ```

        **Keywords:**
        ```plaintext
        ["Contrastive Learning", "Invariant Space"]
        ```
        <|im_end|>
<|im_start|>assistant

2025-06-05 21:48:25,607 [INFO] __main__ - ----------------------------------
2025-06-05 21:48:25,607 [INFO] __main__ - Using cuda:0 to run LMRSD task: idea on gemma-3-1b-it
2025-06-05 21:48:25,607 [INFO] __main__ - ----------------------------------
2025-06-05 21:48:27,174 [INFO] accelerate.utils.modeling - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-06-05 21:48:27,758 [INFO] __main__ - Model-tokenizer Load Time:, 2.1507620811462402 seconds
2025-06-05 21:48:27,758 [INFO] __main__ - ----------------------------------
2025-06-05 21:48:27,759 [INFO] __main__ - Using batch_size=176.0 for gemma-3-1b-it
2025-06-05 21:48:27,759 [INFO] __main__ - BOS token-id: 2
2025-06-05 21:48:27,759 [INFO] __main__ - EOS token-id: 1
2025-06-05 21:48:27,759 [INFO] __main__ - PAD token-id: 0
  0%|          | 0/160 [00:00<?, ?it/s]  1%|          | 1/160 [01:51<4:54:26, 111.11s/it]/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
  1%|▏         | 2/160 [05:43<8:00:24, 182.43s/it]  2%|▏         | 3/160 [08:53<8:06:58, 186.10s/it]  2%|▎         | 4/160 [12:01<8:05:08, 186.59s/it]  3%|▎         | 5/160 [15:08<8:02:10, 186.65s/it]  4%|▍         | 6/160 [18:17<8:01:54, 187.76s/it]  4%|▍         | 7/160 [21:24<7:58:09, 187.51s/it]  5%|▌         | 8/160 [24:32<7:55:05, 187.54s/it]  6%|▌         | 9/160 [27:40<7:51:54, 187.52s/it]  6%|▋         | 10/160 [30:47<7:48:46, 187.51s/it]  7%|▋         | 11/160 [33:54<7:45:13, 187.34s/it]  8%|▊         | 12/160 [37:02<7:42:16, 187.41s/it]  8%|▊         | 13/160 [40:08<7:38:40, 187.21s/it]  9%|▉         | 14/160 [43:16<7:35:53, 187.35s/it]  9%|▉         | 15/160 [46:23<7:32:33, 187.26s/it] 10%|█         | 16/160 [49:30<7:29:24, 187.26s/it] 11%|█         | 17/160 [52:54<7:37:50, 192.10s/it] 11%|█▏        | 18/160 [56:05<7:34:04, 191.87s/it] 12%|█▏        | 19/160 [59:14<7:28:35, 190.89s/it] 12%|█▎        | 20/160 [1:02:40<7:36:00, 195.43s/it] 13%|█▎        | 21/160 [1:05:54<7:32:04, 195.14s/it] 14%|█▍        | 22/160 [1:09:10<7:29:17, 195.34s/it] 14%|█▍        | 23/160 [1:12:24<7:24:58, 194.88s/it] 15%|█▌        | 24/160 [1:15:40<7:22:39, 195.29s/it] 16%|█▌        | 25/160 [1:18:50<7:15:49, 193.70s/it] 16%|█▋        | 26/160 [1:22:00<7:10:22, 192.71s/it] 17%|█▋        | 27/160 [1:25:10<7:05:21, 191.89s/it] 18%|█▊        | 28/160 [1:28:21<7:01:06, 191.41s/it] 18%|█▊        | 29/160 [1:31:30<6:56:52, 190.93s/it] 19%|█▉        | 30/160 [1:34:46<6:56:40, 192.31s/it] 19%|█▉        | 31/160 [1:37:56<6:52:13, 191.73s/it] 20%|██        | 32/160 [1:41:07<6:48:15, 191.37s/it] 21%|██        | 33/160 [1:44:18<6:44:57, 191.32s/it] 21%|██▏       | 34/160 [1:47:28<6:41:07, 191.01s/it] 22%|██▏       | 35/160 [1:50:39<6:37:57, 191.02s/it] 22%|██▎       | 36/160 [1:53:50<6:34:18, 190.79s/it] 23%|██▎       | 37/160 [1:57:01<6:31:12, 190.83s/it] 24%|██▍       | 38/160 [2:00:14<6:29:31, 191.57s/it] 24%|██▍       | 39/160 [2:03:24<6:25:43, 191.27s/it] 25%|██▌       | 40/160 [2:06:34<6:21:46, 190.89s/it] 26%|██▌       | 41/160 [2:09:45<6:18:27, 190.82s/it] 26%|██▋       | 42/160 [2:12:55<6:14:41, 190.52s/it] 27%|██▋       | 43/160 [2:16:05<6:11:11, 190.35s/it] 28%|██▊       | 44/160 [2:19:15<6:08:10, 190.44s/it] 28%|██▊       | 45/160 [2:22:25<6:04:42, 190.28s/it] 29%|██▉       | 46/160 [2:25:36<6:01:47, 190.42s/it] 29%|██▉       | 47/160 [2:28:46<5:58:27, 190.33s/it] 30%|███       | 48/160 [2:31:57<5:55:39, 190.53s/it] 31%|███       | 49/160 [2:35:08<5:52:21, 190.46s/it] 31%|███▏      | 50/160 [2:38:19<5:49:26, 190.60s/it] 32%|███▏      | 51/160 [2:41:29<5:46:07, 190.53s/it] 32%|███▎      | 52/160 [2:44:40<5:43:18, 190.73s/it] 33%|███▎      | 53/160 [2:47:50<5:39:52, 190.58s/it] 34%|███▍      | 54/160 [2:51:01<5:36:32, 190.49s/it] 34%|███▍      | 55/160 [2:54:12<5:33:35, 190.63s/it] 35%|███▌      | 56/160 [2:57:22<5:30:23, 190.61s/it] 36%|███▌      | 57/160 [3:00:33<5:27:23, 190.71s/it] 36%|███▋      | 58/160 [3:03:43<5:23:57, 190.56s/it] 37%|███▋      | 59/160 [3:06:54<5:20:54, 190.64s/it] 38%|███▊      | 60/160 [3:10:07<5:18:58, 191.39s/it] 38%|███▊      | 61/160 [3:13:18<5:15:31, 191.22s/it] 39%|███▉      | 62/160 [3:16:28<5:11:46, 190.88s/it] 39%|███▉      | 63/160 [3:19:39<5:08:29, 190.82s/it] 40%|████      | 64/160 [3:22:49<5:04:58, 190.61s/it] 41%|████      | 65/160 [3:26:00<5:01:49, 190.63s/it] 41%|████▏     | 66/160 [3:29:09<4:58:17, 190.40s/it] 42%|████▏     | 67/160 [3:32:19<4:54:55, 190.27s/it] 42%|████▎     | 68/160 [3:35:30<4:51:54, 190.37s/it] 43%|████▎     | 69/160 [3:38:40<4:48:35, 190.28s/it] 44%|████▍     | 70/160 [3:41:51<4:45:35, 190.39s/it] 44%|████▍     | 71/160 [3:45:01<4:42:19, 190.33s/it] 45%|████▌     | 72/160 [3:48:12<4:39:19, 190.45s/it] 45%|████▌     | 72/160 [3:48:19<4:39:04, 190.27s/it]
Traceback (most recent call last):
  File "/home/ubuntu/llms_LMRSD_baseline.py", line 474, in <module>
    lmrsd_zero_shot_eval(task=LMRSD_TASK, dataset=dataset)
  File "/home/ubuntu/llms_LMRSD_baseline.py", line 391, in lmrsd_zero_shot_eval
    outputs = model.generate(**input_encoded,
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/transformers/generation/utils.py", line 2623, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/transformers/generation/utils.py", line 3604, in _sample
    outputs = self(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 714, in forward
    outputs: BaseModelOutputWithPast = self.model(
                                       ^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/transformers/utils/generic.py", line 943, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 598, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 405, in forward
    hidden_states, self_attn_weights = self.self_attn(
                                       ^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 350, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/transformers/models/gemma3/modeling_gemma3.py", line 280, in eager_attention_forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/miniconda3/envs/mlx/lib/python3.12/site-packages/torch/nn/functional.py", line 2142, in softmax
    ret = input.softmax(dim, dtype=dtype)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.63 GiB. GPU 0 has a total capacity of 94.50 GiB of which 16.30 GiB is free. Including non-PyTorch memory, this process has 78.19 GiB memory in use. Of the allocated memory 70.50 GiB is allocated by PyTorch, with 10.64 GiB allocated in private pools (e.g., CUDA Graphs), and 6.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
